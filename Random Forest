import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

from preprocessing import preprocess  

# Set style for visualizations
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# Load data
df = pd.read_csv('Datasets/train.csv')

# Preprocess train set (fit encoders)
df_processed, encoders = preprocess(df, fit=True)

# === Select features ===
feature_cols = [
    col for col in df_processed.columns 
    if col not in ['price', 'price_encoded'] and df_processed[col].dtype != 'object'
]

X = df_processed[feature_cols]
y = df_processed['price_encoded']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ============================================
# RANDOM FOREST MODEL
# ============================================

print("="*50)
print("RANDOM FOREST MODEL")
print("="*50)

# Note: Random Forest usually doesn't need scaling
# But let's keep it for consistency with previous models

# Initialize Random Forest with default parameters
rf_model = RandomForestClassifier(
    n_estimators=200,  # Number of trees in the forest
    random_state=42,
    n_jobs=-1  ,
    max_depth= 20, 
    min_samples_split= 5
)

# Train the model
rf_model.fit(X_train, y_train)

# Evaluation on validation set
pred = rf_model.predict(X_test)
print("\nVALIDATION SET EVALUATION:")
print("="*30)
print("Accuracy:", accuracy_score(y_test, pred))
print("\nClassification Report:")
print(classification_report(y_test, pred))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, pred))

# ============================================
# FEATURE IMPORTANCE VISUALIZATION
# ============================================

print("\n" + "="*50)
print("FEATURE IMPORTANCE")
print("="*50)

# Get feature importances
importances = rf_model.feature_importances_
feature_names = X_train.columns

# Create DataFrame for feature importances
feature_importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': importances
}).sort_values('importance', ascending=False)

print("\nTop 20 Most Important Features:")
print(feature_importance_df.head(20).to_string())

# Visualize feature importances
plt.figure(figsize=(12, 8))
top_features = feature_importance_df.head(15)
plt.barh(top_features['feature'], top_features['importance'])
plt.xlabel('Feature Importance')
plt.title('Top 15 Feature Importances - Random Forest')
plt.gca().invert_yaxis()  # Most important at top
plt.tight_layout()
plt.show()

# ============================================
# EVALUATE ON TEST.CSV FILE
# ============================================

testdf = pd.read_csv("Datasets/test.csv")

# Extract raw labels
yt = testdf['price']

# Encode labels
yt_encoded = yt.map({'expensive': 1, 'non-expensive': 0})

# Remove label column
testdf_features = testdf.drop(columns=['price'])

# Preprocess using existing encoders
test_processed, _ = preprocess(
    testdf_features,
    label_encoders=encoders,
    fit=False
)

# Make sure test_processed contains ALL training features
for col in feature_cols:
    if col not in test_processed.columns:
        print(f"[WARN] Missing in test set -> adding column: {col}")
        test_processed[col] = 0

# Ensure same column order as training
X_test_final = test_processed[feature_cols]

# Predict on test.csv
y_pred = rf_model.predict(X_test_final)

# Evaluate
print("\n" + "="*50)
print("TEST.CSV EVALUATION")
print("="*50)
print("Accuracy:", accuracy_score(yt_encoded, y_pred))
print("\nClassification Report:")
print(classification_report(yt_encoded, y_pred))

print("\nConfusion Matrix:")
print(confusion_matrix(yt_encoded, y_pred))

# ============================================
# ADDITIONAL RANDOM FOREST ANALYSIS
# ============================================

print("\n" + "="*50)
print("RANDOM FOREST ANALYSIS")
print("="*50)

# Cross-validation score
print("\nCross-validation scores:")
cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='accuracy')
print(f"CV Scores: {cv_scores}")
print(f"Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

# Get predictions probabilities
y_pred_proba = rf_model.predict_proba(X_test_final)[:, 1]




'''

# ============================================
# HYPERPARAMETER TUNING WITH GRIDSEARCHCV
# ============================================

print("\n" + "="*50)
print("GRID SEARCH HYPERPARAMETER TUNING")
print("="*50)

from sklearn.model_selection import GridSearchCV

# Define parameter grid
param_grid = {
    'n_estimators': [50, 100, 200, 300],
    'max_depth': [5, 10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None]
}

print(f"Total parameter combinations: {len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf']) * len(param_grid['max_features'])}")
print("This may take a while...")

# Create GridSearchCV object
grid_search = GridSearchCV(
    estimator=RandomForestClassifier(random_state=42, n_jobs=-1),
    param_grid=param_grid,
    cv=3,  # 3-fold cross-validation
    scoring='accuracy',
    n_jobs=-1,  # Use all CPU cores
    verbose=1  # Show progress
)

# Fit grid search
print("\nStarting Grid Search...")
grid_search.fit(X_train, y_train)

# Get results
print(f"\nBest parameters found: {grid_search.best_params_}")
print(f"Best cross-validation accuracy: {grid_search.best_score_:.4f}")

# Evaluate best model on validation set
best_rf = grid_search.best_estimator_
y_pred_val = best_rf.predict(X_test)
val_accuracy = accuracy_score(y_test, y_pred_val)
print(f"Best model validation accuracy: {val_accuracy:.4f}")

# Evaluate on test.csv
y_pred_grid = best_rf.predict(X_test_final)
grid_accuracy = accuracy_score(yt_encoded, y_pred_grid)
print(f"Best model test.csv accuracy: {grid_accuracy:.4f}")
print(f"Improvement over default: {grid_accuracy - accuracy_score(yt_encoded, y_pred):.4f}")

# Display all results
results_df = pd.DataFrame(grid_search.cv_results_)
print("\nTop 10 parameter combinations:")
print(results_df[['params', 'mean_test_score', 'std_test_score']]
      .sort_values('mean_test_score', ascending=False)
      .head(10)
      .to_string())

# Plot parameter performance
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Plot 1: n_estimators vs accuracy
n_estimators_results = results_df.groupby('param_n_estimators')['mean_test_score'].mean()
axes[0, 0].plot(n_estimators_results.index.astype(str), n_estimators_results.values, 'o-')
axes[0, 0].set_xlabel('n_estimators')
axes[0, 0].set_ylabel('Mean CV Accuracy')
axes[0, 0].set_title('Effect of n_estimators on Accuracy')
axes[0, 0].tick_params(axis='x', rotation=45)

# Plot 2: max_depth vs accuracy
max_depth_results = results_df.groupby('param_max_depth')['mean_test_score'].mean()
axes[0, 1].plot(max_depth_results.index.astype(str), max_depth_results.values, 'o-', color='orange')
axes[0, 1].set_xlabel('max_depth')
axes[0, 1].set_ylabel('Mean CV Accuracy')
axes[0, 1].set_title('Effect of max_depth on Accuracy')
axes[0, 1].tick_params(axis='x', rotation=45)

# Plot 3: min_samples_split vs accuracy
min_split_results = results_df.groupby('param_min_samples_split')['mean_test_score'].mean()
axes[1, 0].bar(min_split_results.index.astype(str), min_split_results.values, color='green')
axes[1, 0].set_xlabel('min_samples_split')
axes[1, 0].set_ylabel('Mean CV Accuracy')
axes[1, 0].set_title('Effect of min_samples_split on Accuracy')

# Plot 4: Feature importance comparison
axes[1, 1].barh(feature_importance_df.head(10)['feature'], 
                feature_importance_df.head(10)['importance'],
                alpha=0.6, label='Default Model')
best_importances = pd.DataFrame({
    'feature': feature_names,
    'importance': best_rf.feature_importances_
}).sort_values('importance', ascending=False).head(10)
axes[1, 1].barh(best_importances['feature'], 
                best_importances['importance'],
                alpha=0.6, label='Tuned Model')
axes[1, 1].set_xlabel('Feature Importance')
axes[1, 1].set_title('Feature Importance: Default vs Tuned')
axes[1, 1].legend()
axes[1, 1].invert_yaxis()

plt.tight_layout()
plt.show()
'''